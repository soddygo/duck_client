//! # ä¸‹è½½æ¨¡å—
//!
//! æä¾›ç»Ÿä¸€çš„æ–‡ä»¶ä¸‹è½½æ¥å£ï¼Œæ”¯æŒï¼š
//! - æ™®é€š HTTP ä¸‹è½½
//! - é˜¿é‡Œäº‘ OSS å…¬ç½‘æ–‡ä»¶ä¸‹è½½ï¼ˆæ‰©å±•è¶…æ—¶ï¼‰
//! - **æ–­ç‚¹ç»­ä¼ ä¸‹è½½** â­
//! - è¿›åº¦å›è°ƒå’Œç›‘æ§
//! - æ–‡ä»¶å®Œæ•´æ€§éªŒè¯
//! - æ™ºèƒ½ç¼“å­˜å’Œæ–­ç‚¹ç»­ä¼ 
//!
//! ## ä¸»è¦ç‰¹æ€§
//!
//! ### æ™ºèƒ½ä¸‹è½½ç­–ç•¥
//! - è‡ªåŠ¨æ£€æµ‹ä¸‹è½½æ–¹å¼ï¼ˆHTTP/æ‰©å±•è¶…æ—¶HTTPï¼‰
//! - æ”¯æŒé˜¿é‡Œäº‘ OSS å¤§æ–‡ä»¶ä¸‹è½½ï¼ˆå…¬ç½‘è®¿é—®ï¼‰
//! - æ‰©å±•è¶…æ—¶æ—¶é—´é¿å…å¤§æ–‡ä»¶ä¸‹è½½å¤±è´¥
//! - **æ™ºèƒ½æ–­ç‚¹ç»­ä¼ ** - è‡ªåŠ¨æ£€æµ‹å·²ä¸‹è½½éƒ¨åˆ†ï¼Œä»ä¸­æ–­ç‚¹ç»§ç»­
//!
//! ### è¿›åº¦ç›‘æ§
//! - å®æ—¶ä¸‹è½½è¿›åº¦å›è°ƒ
//! - ä¸‹è½½é€Ÿåº¦è®¡ç®—
//! - å‰©ä½™æ—¶é—´ä¼°ç®—
//!
//! ### æ–‡ä»¶å®Œæ•´æ€§
//! - SHA-256 å“ˆå¸ŒéªŒè¯
//! - æŸåæ–‡ä»¶è‡ªåŠ¨é‡è¯•
//! - å®Œæ•´æ€§æ ¡éªŒç¼“å­˜
//!
//! ### æ–­ç‚¹ç»­ä¼ 
//! - HTTP Range è¯·æ±‚æ”¯æŒ
//! - è‡ªåŠ¨æ£€æµ‹å·²ä¸‹è½½éƒ¨åˆ†
//! - æ™ºèƒ½æ–‡ä»¶å®Œæ•´æ€§éªŒè¯
//! - æ”¯æŒå¤§æ–‡ä»¶ä¸‹è½½æ¢å¤

use crate::error::{DuckError, Result};
use futures::stream::StreamExt;
use reqwest::Client;
use serde::{Deserialize, Serialize};
use sha2::{Digest, Sha256};
use std::path::Path;
use std::time::Duration;
use tokio::fs::{File, OpenOptions};
use tokio::io::{AsyncReadExt, AsyncWriteExt};
use tracing::{info, warn};
use chrono;

/// ä¸‹è½½è¿›åº¦çŠ¶æ€æšä¸¾
#[derive(Debug, Clone)]
pub enum DownloadStatus {
    Starting,
    Downloading,
    Resuming, // æ–­ç‚¹ç»­ä¼ çŠ¶æ€ â­
    Paused,
    Completed,
    Failed(String),
}

/// ä¸‹è½½è¿›åº¦ä¿¡æ¯
#[derive(Debug, Clone)]
pub struct DownloadProgress {
    pub task_id: String,
    pub file_name: String,
    pub downloaded_bytes: u64,
    pub total_bytes: u64,
    pub download_speed: f64, // bytes/sec
    pub eta_seconds: u64,
    pub percentage: f64,
    pub status: DownloadStatus,
}

/// ä¸‹è½½ä»»åŠ¡å…ƒæ•°æ® â­
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DownloadMetadata {
    pub url: String,
    pub expected_size: u64,
    pub expected_hash: Option<String>,
    pub downloaded_bytes: u64,
    pub start_time: String,
    pub last_update: String,
    pub version: String, // ä¸‹è½½ä»»åŠ¡ç‰ˆæœ¬ï¼Œç”¨äºåŒºåˆ†ä¸åŒçš„ä¸‹è½½
}

impl DownloadMetadata {
    /// åˆ›å»ºæ–°çš„ä¸‹è½½å…ƒæ•°æ®
    pub fn new(url: String, expected_size: u64, expected_hash: Option<String>, version: String) -> Self {
        let now = chrono::Utc::now().to_rfc3339();
        Self {
            url,
            expected_size,
            expected_hash,
            downloaded_bytes: 0,
            start_time: now.clone(),
            last_update: now,
            version,
        }
    }
    
    /// æ›´æ–°ä¸‹è½½è¿›åº¦
    pub fn update_progress(&mut self, downloaded_bytes: u64) {
        self.downloaded_bytes = downloaded_bytes;
        self.last_update = chrono::Utc::now().to_rfc3339();
    }
    
    /// æ£€æŸ¥æ˜¯å¦ä¸ºç›¸åŒçš„ä¸‹è½½ä»»åŠ¡
    pub fn is_same_task(&self, url: &str, expected_size: u64, version: &str) -> bool {
        self.url == url && 
        self.expected_size == expected_size && 
        self.version == version
    }
}

/// ä¸‹è½½å™¨ç±»å‹
#[derive(Debug, Clone)]
pub enum DownloaderType {
    Http,
    HttpExtendedTimeout,
}

/// ä¸‹è½½å™¨é…ç½®
#[derive(Debug, Clone)]
pub struct DownloaderConfig {
    pub timeout_seconds: u64,
    pub chunk_size: usize,
    pub retry_count: u32,
    pub enable_progress_logging: bool,
    pub enable_resume: bool, // å¯ç”¨æ–­ç‚¹ç»­ä¼  â­
    pub resume_threshold: u64, // æ–­ç‚¹ç»­ä¼ é˜ˆå€¼ï¼ˆå­—èŠ‚ï¼‰ï¼Œå°äºæ­¤å€¼çš„æ–‡ä»¶é‡æ–°ä¸‹è½½ â­
    pub progress_interval_seconds: u64, // è¿›åº¦æ˜¾ç¤ºæ—¶é—´é—´éš”ï¼ˆç§’ï¼‰â­
    pub progress_bytes_interval: u64, // è¿›åº¦æ˜¾ç¤ºå­—èŠ‚é—´éš” â­
    pub enable_metadata: bool, // å¯ç”¨å…ƒæ•°æ®ç®¡ç† â­
}

impl Default for DownloaderConfig {
    fn default() -> Self {
        Self {
            timeout_seconds: 30 * 60, // 30åˆ†é’Ÿ
            chunk_size: 8192,         // 8KB
            retry_count: 3,
            enable_progress_logging: true,
            enable_resume: true,      // é»˜è®¤å¯ç”¨æ–­ç‚¹ç»­ä¼  â­
            resume_threshold: 1024 * 1024, // 1MBï¼Œå°äº1MBçš„æ–‡ä»¶é‡æ–°ä¸‹è½½ â­
            progress_interval_seconds: 5, // æ¯5ç§’æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦ â­
            progress_bytes_interval: 50 * 1024 * 1024, // æ¯50MBæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦ â­
            enable_metadata: true,    // é»˜è®¤å¯ç”¨å…ƒæ•°æ®ç®¡ç† â­
        }
    }
}

/// æ–‡ä»¶ä¸‹è½½å™¨
pub struct FileDownloader {
    config: DownloaderConfig,
    client: Client,
}

impl FileDownloader {
    /// åˆ›å»ºæ–°çš„æ–‡ä»¶ä¸‹è½½å™¨
    pub fn new(config: DownloaderConfig) -> Self {
        let client = Client::builder()
            .timeout(Duration::from_secs(config.timeout_seconds))
            .build()
            .expect("Failed to create HTTP client");

        Self { config, client }
    }

    /// åˆ›å»ºé»˜è®¤é…ç½®çš„ä¸‹è½½å™¨
    pub fn default() -> Self {
        Self::new(DownloaderConfig::default())
    }

    /// æ£€æŸ¥ URL æ˜¯å¦ä¸ºé˜¿é‡Œäº‘ OSS é“¾æ¥
    pub fn is_aliyun_oss_url(&self, url: &str) -> bool {
        url.starts_with("https://") && url.contains("aliyuncs.com") && url.contains("oss-")
    }

    /// åˆ¤æ–­ä¸‹è½½å™¨ç±»å‹
    pub fn get_downloader_type(&self, url: &str) -> DownloaderType {
        if self.is_aliyun_oss_url(url) {
            // æ‰€æœ‰é˜¿é‡Œäº‘ OSS URL éƒ½ä½¿ç”¨æ‰©å±•è¶…æ—¶ HTTP ä¸‹è½½ï¼ˆå…¬ç½‘è®¿é—®ï¼‰
            DownloaderType::HttpExtendedTimeout
        } else {
            DownloaderType::Http
        }
    }

    /// æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦æ”¯æŒRangeè¯·æ±‚ â­
    async fn check_range_support(&self, url: &str) -> Result<(bool, u64)> {
        let response = self.client.head(url)
            .send()
            .await
            .map_err(|e| DuckError::custom(format!("æ£€æŸ¥Rangeæ”¯æŒå¤±è´¥: {}", e)))?;

        if !response.status().is_success() {
            return Err(DuckError::custom(format!(
                "æœåŠ¡å™¨å“åº”é”™è¯¯: HTTP {}",
                response.status()
            )));
        }

        let total_size = response.content_length().unwrap_or(0);
        let supports_range = response
            .headers()
            .get("accept-ranges")
            .and_then(|v| v.to_str().ok())
            .map(|v| v.contains("bytes"))
            .unwrap_or(false);

        Ok((supports_range, total_size))
    }

    /// è·å–ä¸‹è½½å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„ â­
    fn get_metadata_path(&self, download_path: &Path) -> std::path::PathBuf {
        download_path.with_extension("download")
    }

    /// ä¿å­˜ä¸‹è½½å…ƒæ•°æ® â­
    async fn save_metadata(&self, download_path: &Path, metadata: &DownloadMetadata) -> Result<()> {
        self.save_metadata_with_logging(download_path, metadata, true).await
    }

    /// ä¿å­˜ä¸‹è½½å…ƒæ•°æ®ï¼ˆå¯æ§åˆ¶æ—¥å¿—è¾“å‡ºï¼‰â­
    async fn save_metadata_with_logging(&self, download_path: &Path, metadata: &DownloadMetadata, show_log: bool) -> Result<()> {
        if !self.config.enable_metadata {
            return Ok(());
        }

        let metadata_path = self.get_metadata_path(download_path);
        let json_content = serde_json::to_string_pretty(metadata)
            .map_err(|e| DuckError::custom(format!("åºåˆ—åŒ–å…ƒæ•°æ®å¤±è´¥: {}", e)))?;

        tokio::fs::write(&metadata_path, json_content)
            .await
            .map_err(|e| DuckError::custom(format!("ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {}", e)))?;

        if show_log {
            info!("ğŸ’¾ å·²ä¿å­˜ä¸‹è½½å…ƒæ•°æ®: {}", metadata_path.display());
        }
        Ok(())
    }

    /// åŠ è½½ä¸‹è½½å…ƒæ•°æ® â­
    async fn load_metadata(&self, download_path: &Path) -> Result<Option<DownloadMetadata>> {
        if !self.config.enable_metadata {
            return Ok(None);
        }

        let metadata_path = self.get_metadata_path(download_path);
        if !metadata_path.exists() {
            return Ok(None);
        }

        let content = tokio::fs::read_to_string(&metadata_path)
            .await
            .map_err(|e| DuckError::custom(format!("è¯»å–å…ƒæ•°æ®å¤±è´¥: {}", e)))?;

        let metadata: DownloadMetadata = serde_json::from_str(&content)
            .map_err(|e| DuckError::custom(format!("è§£æå…ƒæ•°æ®å¤±è´¥: {}", e)))?;

        info!("ğŸ“‹ å·²åŠ è½½ä¸‹è½½å…ƒæ•°æ®: {}", metadata_path.display());
        Ok(Some(metadata))
    }

    /// æ¸…ç†ä¸‹è½½å…ƒæ•°æ® â­
    async fn cleanup_metadata(&self, download_path: &Path) -> Result<()> {
        if !self.config.enable_metadata {
            return Ok(());
        }

        let metadata_path = self.get_metadata_path(download_path);
        if metadata_path.exists() {
            tokio::fs::remove_file(&metadata_path)
                .await
                .map_err(|e| DuckError::custom(format!("æ¸…ç†å…ƒæ•°æ®å¤±è´¥: {}", e)))?;
            info!("ğŸ§¹ å·²æ¸…ç†ä¸‹è½½å…ƒæ•°æ®: {}", metadata_path.display());
        }
        Ok(())
    }

    /// æ™ºèƒ½æ£€æŸ¥æ–­ç‚¹ç»­ä¼ å¯è¡Œæ€§ â­
    async fn check_resume_feasibility(
        &self, 
        download_path: &Path, 
        url: &str,
        total_size: u64,
        expected_hash: Option<&str>,
        version: &str
    ) -> Result<Option<u64>> {
        info!("ğŸ” æ£€æŸ¥æ–­ç‚¹ç»­ä¼ å¯è¡Œæ€§...");
        
        // 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if !download_path.exists() {
            info!("ğŸ“ ç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•ç»­ä¼ ");
            return Ok(None);
        }

        // 2. è·å–å½“å‰æ–‡ä»¶å¤§å°
        let file_metadata = tokio::fs::metadata(download_path)
            .await
            .map_err(|e| DuckError::custom(format!("è¯»å–æ–‡ä»¶å…ƒæ•°æ®å¤±è´¥: {}", e)))?;
        let existing_size = file_metadata.len();

        info!("ğŸ“Š å½“å‰æ–‡ä»¶å¤§å°: {} bytes ({:.2} MB)", existing_size, existing_size as f64 / 1024.0 / 1024.0);

        // 3. ã€ä¼˜å…ˆã€‘æ£€æŸ¥hashæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™ä¼˜å…ˆéªŒè¯hash â­
        if let Some(expected_hash) = expected_hash {
            info!("ğŸ” ä¼˜å…ˆè¿›è¡ŒhashéªŒè¯...");
            match Self::calculate_file_hash(download_path).await {
                Ok(actual_hash) => {
                    if actual_hash.to_lowercase() == expected_hash.to_lowercase() {
                        info!("âœ… æ–‡ä»¶hashéªŒè¯é€šè¿‡ï¼Œæ–‡ä»¶å·²å®Œæ•´");
                        // æ¸…ç†å…ƒæ•°æ®ï¼ˆä¸‹è½½å·²å®Œæˆï¼‰
                        let _ = self.cleanup_metadata(download_path).await;
                        return Ok(None); // æ— éœ€ä¸‹è½½
                    } else {
                        info!("âŒ æ–‡ä»¶hashéªŒè¯å¤±è´¥ï¼Œè¿›å…¥æ–­ç‚¹ç»­ä¼ åˆ¤æ–­");
                        info!("   æœŸæœ›hash: {}", expected_hash);
                        info!("   å®é™…hash: {}", actual_hash);
                        // ç»§ç»­ä¸‹é¢çš„æ–­ç‚¹ç»­ä¼ é€»è¾‘ï¼Œä¸è¦ç«‹å³åˆ é™¤æ–‡ä»¶
                    }
                }
                Err(e) => {
                    warn!("âš ï¸ è®¡ç®—æ–‡ä»¶hashå¤±è´¥: {}ï¼Œè¿›å…¥æ–­ç‚¹ç»­ä¼ åˆ¤æ–­", e);
                    // ç»§ç»­ä¸‹é¢çš„æ–­ç‚¹ç»­ä¼ é€»è¾‘
                }
            }
        }

        // 4. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å®Œæ•´ï¼ˆå¤§å°æ£€æŸ¥ï¼‰
        if existing_size >= total_size {
            // å¦‚æœæ–‡ä»¶å¤§å°å·²å®Œæ•´ä½†hashä¸åŒ¹é…ï¼Œè¯´æ˜æ–‡ä»¶æŸåï¼Œé‡æ–°ä¸‹è½½
            if expected_hash.is_some() {
                warn!("âŒ æ–‡ä»¶å¤§å°å®Œæ•´ä½†hashä¸åŒ¹é…ï¼Œæ–‡ä»¶å·²æŸåï¼Œå°†é‡æ–°ä¸‹è½½");
                let _ = tokio::fs::remove_file(download_path).await;
                let _ = self.cleanup_metadata(download_path).await;
                return Ok(None); // é‡æ–°ä¸‹è½½
            } else {
                // æ²¡æœ‰hashéªŒè¯ï¼Œè®¤ä¸ºæ–‡ä»¶å®Œæ•´
                info!("âœ… æ–‡ä»¶å¤§å°å®Œæ•´ä¸”æ— hashéªŒè¯è¦æ±‚ï¼Œè®¤ä¸ºæ–‡ä»¶å®Œæ•´");
                let _ = self.cleanup_metadata(download_path).await;
                return Ok(None);
            }
        }

        // 5. æ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦ç¬¦åˆç»­ä¼ é˜ˆå€¼
        if existing_size < self.config.resume_threshold {
            info!("ğŸ“ æ–‡ä»¶è¿‡å° ({} bytes < {} bytes)ï¼Œå°†é‡æ–°ä¸‹è½½", 
                existing_size, self.config.resume_threshold);
            let _ = tokio::fs::remove_file(download_path).await;
            let _ = self.cleanup_metadata(download_path).await;
            return Ok(None);
        }

        // 6. æ£€æŸ¥ä¸‹è½½å…ƒæ•°æ®
        if let Some(metadata) = self.load_metadata(download_path).await? {
            // éªŒè¯æ˜¯å¦ä¸ºç›¸åŒçš„ä¸‹è½½ä»»åŠ¡
            if metadata.is_same_task(url, total_size, version) {
                info!("âœ… å‘ç°åŒ¹é…çš„ä¸‹è½½ä»»åŠ¡");
                info!("   åŸå§‹URL: {}", metadata.url);
                info!("   é¢„æœŸå¤§å°: {} bytes", metadata.expected_size);
                info!("   å¼€å§‹æ—¶é—´: {}", metadata.start_time);
                info!("   ä¸Šæ¬¡æ›´æ–°: {}", metadata.last_update);
                
                // å¦‚æœæœ‰hashè¦æ±‚ï¼Œé¢å¤–æ£€æŸ¥hashæ˜¯å¦åŒ¹é…
                if let Some(expected_hash) = expected_hash {
                    if let Some(ref metadata_hash) = metadata.expected_hash {
                        if metadata_hash.to_lowercase() == expected_hash.to_lowercase() {
                            info!("âœ… å…ƒæ•°æ®hashåŒ¹é…ï¼Œå¯ä»¥å®‰å…¨ç»­ä¼ ");
                        } else {
                            warn!("âŒ å…ƒæ•°æ®hashä¸åŒ¹é…ï¼Œå¯èƒ½æ˜¯ä¸åŒç‰ˆæœ¬");
                            warn!("   å½“å‰æœŸæœ›hash: {}", expected_hash);
                            warn!("   å…ƒæ•°æ®è®°å½•hash: {}", metadata_hash);
                            warn!("   æ¸…ç†æ—§æ•°æ®ï¼Œé‡æ–°ä¸‹è½½");
                            let _ = tokio::fs::remove_file(download_path).await;
                            let _ = self.cleanup_metadata(download_path).await;
                            return Ok(None);
                        }
                    } else {
                        warn!("âš ï¸ å…ƒæ•°æ®ç¼ºå°‘hashä¿¡æ¯ï¼Œä½†ç°åœ¨éœ€è¦hashéªŒè¯");
                        warn!("   ä¸ºå®‰å…¨èµ·è§ï¼Œé‡æ–°ä¸‹è½½");
                        let _ = tokio::fs::remove_file(download_path).await;
                        let _ = self.cleanup_metadata(download_path).await;
                        return Ok(None);
                    }
                }
                
                // éªŒè¯å…ƒæ•°æ®ä¸­çš„ä¸‹è½½è¿›åº¦æ˜¯å¦ä¸æ–‡ä»¶å¤§å°ä¸€è‡´
                if metadata.downloaded_bytes == existing_size {
                    info!("âœ… å…ƒæ•°æ®ä¸æ–‡ä»¶å¤§å°ä¸€è‡´ï¼Œå¯ä»¥ç»­ä¼ ");
                    info!("ğŸ“ ç»­ä¼ ç‚¹: {} bytes / {} bytes ({:.1}%)", 
                        existing_size, total_size, 
                        (existing_size as f64 / total_size as f64) * 100.0);
                    return Ok(Some(existing_size));
                } else {
                    warn!("âš ï¸ å…ƒæ•°æ®ä¸æ–‡ä»¶å¤§å°ä¸ä¸€è‡´");
                    warn!("   å…ƒæ•°æ®è®°å½•: {} bytes", metadata.downloaded_bytes);
                    warn!("   å®é™…æ–‡ä»¶: {} bytes", existing_size);
                    
                    // ä»¥å®é™…æ–‡ä»¶å¤§å°ä¸ºå‡†ï¼Œæ›´æ–°å…ƒæ•°æ®
                    info!("ğŸ”„ ä»¥å®é™…æ–‡ä»¶å¤§å°ä¸ºå‡†ï¼Œç»§ç»­ç»­ä¼ ");
                    return Ok(Some(existing_size));
                }
            } else {
                warn!("âŒ ä¸‹è½½ä»»åŠ¡ä¸åŒ¹é…ï¼Œå°†é‡æ–°ä¸‹è½½");
                warn!("   å½“å‰URL: {}", url);
                warn!("   å…ƒæ•°æ®URL: {}", metadata.url);
                warn!("   å½“å‰å¤§å°: {} bytes", total_size);
                warn!("   å…ƒæ•°æ®å¤§å°: {} bytes", metadata.expected_size);
                warn!("   å½“å‰ç‰ˆæœ¬: {}", version);
                warn!("   å…ƒæ•°æ®ç‰ˆæœ¬: {}", metadata.version);
                
                // æ¸…ç†ä¸åŒ¹é…çš„ä¸‹è½½
                let _ = tokio::fs::remove_file(download_path).await;
                let _ = self.cleanup_metadata(download_path).await;
                return Ok(None);
            }
        } else {
            // æ²¡æœ‰å…ƒæ•°æ®ï¼Œä½†æ–‡ä»¶å­˜åœ¨ - å¯èƒ½æ˜¯æ—§çš„ä¸‹è½½
            warn!("âš ï¸ å‘ç°æ— å…ƒæ•°æ®çš„éƒ¨åˆ†æ–‡ä»¶");
            
            if expected_hash.is_some() {
                // æœ‰hashè¦æ±‚ï¼Œæ£€æŸ¥æ˜¯å¦å¯ä»¥æ™ºèƒ½ç»­ä¼ 
                info!("ğŸ” æœ‰hashéªŒè¯è¦æ±‚ï¼Œè¯„ä¼°æ™ºèƒ½ç»­ä¼ å¯èƒ½æ€§");
                
                // å¦‚æœæ–‡ä»¶å¤§å°è¶…è¿‡æ€»å¤§å°çš„50%ï¼Œå°è¯•ç»­ä¼ 
                let progress_percentage = (existing_size as f64 / total_size as f64) * 100.0;
                if progress_percentage >= 50.0 {
                    info!("ğŸ“Š æ–‡ä»¶å·²ä¸‹è½½ {:.1}%ï¼Œå°è¯•æ™ºèƒ½ç»­ä¼ ", progress_percentage);
                    info!("   æ³¨æ„ï¼šç»­ä¼ åå°†è¿›è¡Œå®Œæ•´æ€§éªŒè¯");
                    return Ok(Some(existing_size));
                } else {
                    warn!("ğŸ”’ æ–‡ä»¶è¿›åº¦ä¸è¶³50%ä¸”æ— å…ƒæ•°æ®ï¼Œä¸ºå®‰å…¨èµ·è§å°†é‡æ–°ä¸‹è½½");
                    let _ = tokio::fs::remove_file(download_path).await;
                    return Ok(None);
                }
            } else {
                // æ²¡æœ‰hashè¦æ±‚ï¼Œå°è¯•ç»­ä¼ 
                info!("ğŸ¤” å°è¯•ç»­ä¼ æ— å…ƒæ•°æ®çš„æ–‡ä»¶");
                info!("ğŸ“ ç»­ä¼ ç‚¹: {} bytes / {} bytes ({:.1}%)", 
                    existing_size, total_size, 
                    (existing_size as f64 / total_size as f64) * 100.0);
                return Ok(Some(existing_size));
            }
        }
    }

    /// ä¸‹è½½æ–‡ä»¶ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰â­
    pub async fn download_file<F>(
        &self,
        url: &str,
        download_path: &Path,
        progress_callback: Option<F>,
    ) -> Result<()>
    where
        F: Fn(DownloadProgress) + Send + Sync + 'static,
    {
        self.download_file_with_options(url, download_path, progress_callback, None, None).await
    }

    /// ä¸‹è½½æ–‡ä»¶ï¼ˆå¸¦é¢å¤–é€‰é¡¹ï¼‰â­
    pub async fn download_file_with_options<F>(
        &self,
        url: &str,
        download_path: &Path,
        progress_callback: Option<F>,
        expected_hash: Option<&str>,
        version: Option<&str>,
    ) -> Result<()>
    where
        F: Fn(DownloadProgress) + Send + Sync + 'static,
    {
        let downloader_type = self.get_downloader_type(url);
        let version = version.unwrap_or("unknown");
        
        info!("ğŸŒ å¼€å§‹ä¸‹è½½æ–‡ä»¶");
        info!("   URL: {}", url);
        info!("   ç›®æ ‡è·¯å¾„: {}", download_path.display());
        info!("   ä¸‹è½½å™¨ç±»å‹: {:?}", downloader_type);
        info!("   æ–­ç‚¹ç»­ä¼ : {}", if self.config.enable_resume { "å¯ç”¨" } else { "ç¦ç”¨" });
        if let Some(hash) = expected_hash {
            info!("   æœŸæœ›Hash: {}", hash);
        }
        info!("   ç‰ˆæœ¬æ ‡è¯†: {}", version);

        // æ£€æŸ¥Rangeæ”¯æŒå’Œæ–‡ä»¶å¤§å°
        let (supports_range, total_size) = self.check_range_support(url).await?;
        
        if total_size > 0 {
            info!("ğŸ“¦ æœåŠ¡å™¨æ–‡ä»¶å¤§å°: {} bytes ({:.2} MB)", total_size, total_size as f64 / 1024.0 / 1024.0);
        }

        if supports_range && self.config.enable_resume {
            info!("âœ… æœåŠ¡å™¨æ”¯æŒRangeè¯·æ±‚ï¼Œå¯ç”¨æ–­ç‚¹ç»­ä¼ ");
        } else if !supports_range {
            warn!("âš ï¸ æœåŠ¡å™¨ä¸æ”¯æŒRangeè¯·æ±‚ï¼Œä½¿ç”¨æ™®é€šä¸‹è½½");
        }

        // æ™ºèƒ½æ£€æŸ¥æ–­ç‚¹ç»­ä¼ å¯è¡Œæ€§
        let existing_size = if supports_range && self.config.enable_resume {
            self.check_resume_feasibility(download_path, url, total_size, expected_hash, version).await?
        } else {
            None
        };

        // åˆ›å»ºä¸‹è½½å…ƒæ•°æ®
        let mut metadata = DownloadMetadata::new(
            url.to_string(), 
            total_size, 
            expected_hash.map(|s| s.to_string()), 
            version.to_string()
        );

        // å¦‚æœæ˜¯ç»­ä¼ ï¼Œæ›´æ–°è¿›åº¦
        if let Some(resume_size) = existing_size {
            metadata.update_progress(resume_size);
        }

        // ä¿å­˜åˆå§‹å…ƒæ•°æ®
        self.save_metadata(download_path, &metadata).await?;

        // æ‰§è¡Œä¸‹è½½
        let result = match downloader_type {
            DownloaderType::Http => {
                self.download_via_http_with_resume(url, download_path, progress_callback, existing_size, total_size, &mut metadata).await
            }
            DownloaderType::HttpExtendedTimeout => {
                self.download_via_http_extended_timeout_with_resume(url, download_path, progress_callback, existing_size, total_size, &mut metadata).await
            }
        };

        // å¤„ç†ä¸‹è½½ç»“æœ
        match result {
            Ok(_) => {
                // ä¸‹è½½æˆåŠŸï¼Œæ¸…ç†å…ƒæ•°æ®
                info!("ğŸ‰ ä¸‹è½½å®Œæˆï¼Œæ¸…ç†å…ƒæ•°æ®");
                let _ = self.cleanup_metadata(download_path).await;
                
                // æœ€ç»ˆhashéªŒè¯ï¼ˆå¦‚æœæä¾›ï¼‰
                if let Some(hash) = expected_hash {
                    info!("ğŸ” æœ€ç»ˆhashéªŒè¯...");
                    match Self::calculate_file_hash(download_path).await {
                        Ok(actual_hash) => {
                            if actual_hash.to_lowercase() == hash.to_lowercase() {
                                info!("âœ… æœ€ç»ˆhashéªŒè¯é€šè¿‡");
                            } else {
                                warn!("âŒ æœ€ç»ˆhashéªŒè¯å¤±è´¥");
                                warn!("   æœŸæœ›: {}", hash);
                                warn!("   å®é™…: {}", actual_hash);
                                return Err(DuckError::custom("æ–‡ä»¶hashéªŒè¯å¤±è´¥"));
                            }
                        }
                        Err(e) => {
                            warn!("âš ï¸ è®¡ç®—æœ€ç»ˆhashå¤±è´¥: {}", e);
                        }
                    }
                }
                Ok(())
            }
            Err(e) => {
                // ä¸‹è½½å¤±è´¥ï¼Œä¿ç•™å…ƒæ•°æ®ç”¨äºä¸‹æ¬¡ç»­ä¼ 
                warn!("âŒ ä¸‹è½½å¤±è´¥: {}", e);
                info!("ğŸ’¾ ä¿ç•™å…ƒæ•°æ®ç”¨äºä¸‹æ¬¡ç»­ä¼ ");
                Err(e)
            }
        }
    }

    /// ä½¿ç”¨æ™®é€š HTTP ä¸‹è½½ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰â­
    async fn download_via_http_with_resume<F>(
        &self,
        url: &str,
        download_path: &Path,
        progress_callback: Option<F>,
        existing_size: Option<u64>,
        total_size: u64,
        metadata: &mut DownloadMetadata,
    ) -> Result<()>
    where
        F: Fn(DownloadProgress) + Send + Sync + 'static,
    {
        info!("ğŸ“¥ ä½¿ç”¨æ™®é€š HTTP ä¸‹è½½");
        self.download_with_resume_internal(url, download_path, progress_callback, existing_size, total_size, "http_download", metadata).await
    }

    /// ä½¿ç”¨æ‰©å±•è¶…æ—¶çš„ HTTP ä¸‹è½½ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰â­
    async fn download_via_http_extended_timeout_with_resume<F>(
        &self,
        url: &str,
        download_path: &Path,
        progress_callback: Option<F>,
        existing_size: Option<u64>,
        total_size: u64,
        metadata: &mut DownloadMetadata,
    ) -> Result<()>
    where
        F: Fn(DownloadProgress) + Send + Sync + 'static,
    {
        if self.is_aliyun_oss_url(url) {
            info!("ğŸ“¥ ä½¿ç”¨æ‰©å±•è¶…æ—¶ HTTP ä¸‹è½½ (é˜¿é‡Œäº‘ OSS å…¬ç½‘æ–‡ä»¶)");
            info!("   ğŸ’¡ æ£€æµ‹åˆ°å…¬ç½‘è®¿é—®çš„ OSS æ–‡ä»¶ï¼Œæ— éœ€å¯†é’¥");
            if existing_size.is_some() {
                info!("   ğŸ”„ æ”¯æŒæ–­ç‚¹ç»­ä¼ ");
            }
        } else {
            info!("ğŸ“¥ ä½¿ç”¨æ‰©å±•è¶…æ—¶ HTTP ä¸‹è½½");
        }
        
        self.download_with_resume_internal(url, download_path, progress_callback, existing_size, total_size, "extended_http_download", metadata).await
    }

    /// å†…éƒ¨æ–­ç‚¹ç»­ä¼ ä¸‹è½½å®ç° â­
    async fn download_with_resume_internal<F>(
        &self,
        url: &str,
        download_path: &Path,
        progress_callback: Option<F>,
        existing_size: Option<u64>,
        total_size: u64,
        task_id: &str,
        metadata: &mut DownloadMetadata,
    ) -> Result<()>
    where
        F: Fn(DownloadProgress) + Send + Sync + 'static,
    {
        let start_byte = existing_size.unwrap_or(0);
        let is_resume = existing_size.is_some();

        // æ„å»ºè¯·æ±‚
        let mut request = self.client.get(url);
        
        if is_resume {
            info!("ğŸ”„ æ–­ç‚¹ç»­ä¼ ï¼šä»å­—èŠ‚ {} å¼€å§‹ä¸‹è½½", start_byte);
            request = request.header("Range", format!("bytes={}-", start_byte));
        }

        let response = request
            .send()
            .await
            .map_err(|e| DuckError::custom(format!("å‘èµ·ä¸‹è½½è¯·æ±‚å¤±è´¥: {}", e)))?;

        // æ£€æŸ¥å“åº”çŠ¶æ€
        let expected_status = if is_resume { 206 } else { 200 };
        if response.status().as_u16() != expected_status {
            return Err(DuckError::custom(format!(
                "ä¸‹è½½å¤±è´¥: HTTP {} (æœŸæœ›: {})",
                response.status(), expected_status
            )));
        }

        // æ‰“å¼€æ–‡ä»¶ï¼ˆè¿½åŠ æ¨¡å¼æˆ–åˆ›å»ºæ¨¡å¼ï¼‰
        let mut file = if is_resume {
            info!("ğŸ“ ä»¥è¿½åŠ æ¨¡å¼æ‰“å¼€æ–‡ä»¶");
            OpenOptions::new()
                .create(true)
                .append(true)
                .open(download_path)
                .await
                .map_err(|e| DuckError::custom(format!("æ‰“å¼€æ–‡ä»¶å¤±è´¥: {}", e)))?
        } else {
            info!("ğŸ“ åˆ›å»ºæ–°æ–‡ä»¶");
            File::create(download_path)
                .await
                .map_err(|e| DuckError::custom(format!("åˆ›å»ºæ–‡ä»¶å¤±è´¥: {}", e)))?
        };

        // æ‰§è¡Œä¸‹è½½
        self.download_stream_with_resume(
            response, 
            &mut file, 
            download_path, 
            progress_callback, 
            task_id, 
            start_byte, 
            total_size,
            is_resume,
            metadata
        ).await
    }

    /// é€šç”¨çš„æµå¼ä¸‹è½½å¤„ç†ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰â­
    async fn download_stream_with_resume<F>(
        &self,
        response: reqwest::Response,
        file: &mut File,
        download_path: &Path,
        progress_callback: Option<F>,
        task_id: &str,
        start_byte: u64,
        total_size: u64,
        is_resume: bool,
        metadata: &mut DownloadMetadata,
    ) -> Result<()>
    where
        F: Fn(DownloadProgress) + Send + Sync + 'static,
    {
        let mut downloaded = start_byte; // ä»å·²ä¸‹è½½çš„å­—èŠ‚å¼€å§‹è®¡ç®—
        let mut stream = response.bytes_stream();
        let mut last_progress_time = std::time::Instant::now();
        let mut last_progress_bytes = downloaded;
        let progress_interval = std::time::Duration::from_secs(self.config.progress_interval_seconds);
        
        // é¦–æ¬¡è¿›åº¦å›è°ƒ
        if let Some(callback) = progress_callback.as_ref() {
            let status = if is_resume { DownloadStatus::Resuming } else { DownloadStatus::Starting };
            callback(DownloadProgress {
                task_id: task_id.to_string(),
                file_name: download_path.file_name().unwrap_or_default().to_string_lossy().to_string(),
                downloaded_bytes: downloaded,
                total_bytes: total_size,
                download_speed: 0.0,
                eta_seconds: 0,
                percentage: if total_size > 0 { downloaded as f64 / total_size as f64 * 100.0 } else { 0.0 },
                status,
            });
        }
        
        while let Some(chunk) = stream.next().await {
            let chunk = chunk.map_err(|e| DuckError::custom(format!("ä¸‹è½½æ•°æ®å¤±è´¥: {}", e)))?;
            
            file.write_all(&chunk)
                .await
                .map_err(|e| DuckError::custom(format!("å†™å…¥æ–‡ä»¶å¤±è´¥: {}", e)))?;
            
            downloaded += chunk.len() as u64;
            
            // è°ƒç”¨è¿›åº¦å›è°ƒ
            if let Some(callback) = progress_callback.as_ref() {
                let progress = if total_size > 0 {
                    downloaded as f64 / total_size as f64 * 100.0
                } else {
                    0.0
                };
                
                callback(DownloadProgress {
                    task_id: task_id.to_string(),
                    file_name: download_path.file_name().unwrap_or_default().to_string_lossy().to_string(),
                    downloaded_bytes: downloaded,
                    total_bytes: total_size,
                    download_speed: 0.0,
                    eta_seconds: 0,
                    percentage: progress,
                    status: DownloadStatus::Downloading,
                });
            }
            
            // è¿›åº¦æ˜¾ç¤ºé€»è¾‘
            if self.config.enable_progress_logging {
                let now = std::time::Instant::now();
                let bytes_since_last = downloaded - last_progress_bytes;
                let time_since_last = now.duration_since(last_progress_time);
                
                let should_show_progress = 
                    bytes_since_last >= self.config.progress_bytes_interval ||  // æ ¹æ®é…ç½®çš„å­—èŠ‚é—´éš”æ˜¾ç¤º
                    time_since_last >= progress_interval ||  // æ ¹æ®é…ç½®çš„æ—¶é—´é—´éš”æ˜¾ç¤º
                    (total_size > 0 && downloaded >= total_size); // ä¸‹è½½å®Œæˆæ—¶æ˜¾ç¤º
                
                if should_show_progress {
                    if total_size > 0 {
                        let percentage = (downloaded as f64 / total_size as f64 * 100.0) as u32;
                        let status_icon = if is_resume && downloaded <= start_byte + 50*1024*1024 {
                            "ğŸ”„" // æ–­ç‚¹ç»­ä¼ å›¾æ ‡
                        } else {
                            "ğŸ“¥" // æ™®é€šä¸‹è½½å›¾æ ‡
                        };
                        
                        // è®¡ç®—ä¸‹è½½é€Ÿåº¦ï¼ˆä»…ç”¨äºæ˜¾ç¤ºï¼‰
                        let speed_mbps = if time_since_last.as_secs() > 0 {
                            (bytes_since_last as f64 / 1024.0 / 1024.0) / time_since_last.as_secs() as f64
                        } else {
                            0.0
                        };
                        
                        info!("{} ä¸‹è½½è¿›åº¦: {}% ({:.1}/{:.1} MB) é€Ÿåº¦: {:.1} MB/s", 
                            status_icon,
                            percentage,
                            downloaded as f64 / 1024.0 / 1024.0,
                            total_size as f64 / 1024.0 / 1024.0,
                            speed_mbps
                        );
                    } else {
                        info!("ğŸ“¥ å·²ä¸‹è½½: {:.1} MB", downloaded as f64 / 1024.0 / 1024.0);
                    }
                    
                    last_progress_time = now;
                    last_progress_bytes = downloaded;
                    
                    // æ›´æ–°å…ƒæ•°æ®ï¼ˆå‡å°‘ä¿å­˜é¢‘ç‡ï¼Œé¿å…é‡å¤æ—¥å¿—ï¼‰â­
                    if self.config.enable_metadata {
                        metadata.update_progress(downloaded);
                        // åªåœ¨ç‰¹å®šæ¡ä»¶ä¸‹ä¿å­˜å…ƒæ•°æ®ï¼šæ¯500MBæˆ–æ¯5åˆ†é’Ÿ
                        let should_save_metadata = 
                            bytes_since_last >= 500 * 1024 * 1024 ||  // æ¯500MBä¿å­˜ä¸€æ¬¡
                            time_since_last >= std::time::Duration::from_secs(300); // æ¯5åˆ†é’Ÿä¿å­˜ä¸€æ¬¡
                        
                        if should_save_metadata {
                            // é™é»˜ä¿å­˜ï¼Œä¸è¾“å‡ºæ—¥å¿—ï¼ˆé¿å…é‡å¤æ—¥å¿—ï¼‰
                            let _ = self.save_metadata_with_logging(download_path, metadata, false).await;
                        }
                    }
                }
            }
        }
        
        // ç¡®ä¿æ–‡ä»¶å·²åˆ·æ–°åˆ°ç£ç›˜
        file.flush().await
            .map_err(|e| DuckError::custom(format!("åˆ·æ–°æ–‡ä»¶ç¼“å†²åŒºå¤±è´¥: {}", e)))?;
        
        let download_type = if is_resume { "æ–­ç‚¹ç»­ä¼ ä¸‹è½½" } else { "ä¸‹è½½" };
        info!("âœ… {}å®Œæˆ", download_type);
        info!("   æ–‡ä»¶è·¯å¾„: {}", download_path.display());
        info!("   æœ€ç»ˆå¤§å°: {} bytes ({:.2} MB)", downloaded, downloaded as f64 / 1024.0 / 1024.0);
        if is_resume {
            info!("   ç»­ä¼ å¤§å°: {} bytes ({:.2} MB)", downloaded - start_byte, (downloaded - start_byte) as f64 / 1024.0 / 1024.0);
        }
        
        Ok(())
    }

    /// è®¡ç®—æ–‡ä»¶çš„SHA256å“ˆå¸Œå€¼
    pub async fn calculate_file_hash(file_path: &Path) -> Result<String> {
        if !file_path.exists() {
            return Err(DuckError::Custom(format!(
                "æ–‡ä»¶ä¸å­˜åœ¨: {}",
                file_path.display()
            )));
        }

        let mut file = File::open(file_path).await.map_err(|e| {
            DuckError::Custom(format!("æ— æ³•æ‰“å¼€æ–‡ä»¶ {}: {}", file_path.display(), e))
        })?;

        let mut hasher = Sha256::new();
        let mut buffer = vec![0u8; 8192]; // 8KB buffer

        loop {
            let bytes_read = file.read(&mut buffer).await.map_err(|e| {
                DuckError::Custom(format!("è¯»å–æ–‡ä»¶å¤±è´¥ {}: {}", file_path.display(), e))
            })?;

            if bytes_read == 0 {
                break;
            }

            hasher.update(&buffer[..bytes_read]);
        }

        let hash = hasher.finalize();
        Ok(format!("{hash:x}"))
    }

    /// éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
    pub async fn verify_file_integrity(file_path: &Path, expected_hash: &str) -> Result<bool> {
        info!("éªŒè¯æ–‡ä»¶å®Œæ•´æ€§: {}", file_path.display());

        // è®¡ç®—å½“å‰æ–‡ä»¶çš„å“ˆå¸Œå€¼
        let actual_hash = Self::calculate_file_hash(file_path).await?;

        // æ¯”è¾ƒå“ˆå¸Œå€¼ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
        let matches = actual_hash.to_lowercase() == expected_hash.to_lowercase();

        if matches {
            info!("âœ… æ–‡ä»¶å®Œæ•´æ€§éªŒè¯é€šè¿‡: {}", file_path.display());
        } else {
            warn!("âŒ æ–‡ä»¶å®Œæ•´æ€§éªŒè¯å¤±è´¥: {}", file_path.display());
            warn!("   æœŸæœ›å“ˆå¸Œ: {}", expected_hash);
            warn!("   å®é™…å“ˆå¸Œ: {}", actual_hash);
        }

        Ok(matches)
    }
}

/// ç®€åŒ–çš„ä¸‹è½½åŠŸèƒ½ï¼Œç”¨äºå‘åå…¼å®¹
pub async fn download_file_simple(
    url: &str,
    download_path: &Path,
) -> Result<()> {
    let downloader = FileDownloader::default();
    downloader.download_file::<fn(DownloadProgress)>(url, download_path, None).await
}

/// å¸¦è¿›åº¦å›è°ƒçš„ä¸‹è½½åŠŸèƒ½
pub async fn download_file_with_progress<F>(
    url: &str,
    download_path: &Path,
    progress_callback: Option<F>,
) -> Result<()>
where
    F: Fn(DownloadProgress) + Send + Sync + 'static,
{
    let downloader = FileDownloader::default();
    downloader.download_file(url, download_path, progress_callback).await
}

/// åˆ›å»ºè‡ªå®šä¹‰é…ç½®çš„ä¸‹è½½å™¨
pub fn create_downloader(config: DownloaderConfig) -> FileDownloader {
    FileDownloader::new(config)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_aliyun_oss_url_detection() {
        let downloader = FileDownloader::default();
        
        // æµ‹è¯•æ‚¨æä¾›çš„çœŸå®é˜¿é‡Œäº‘ OSS URL
        let real_oss_url = "https://nuwa-packages.oss-rg-china-mainland.aliyuncs.com/duck-client-releases/docker/20250705082538/docker.zip";
        assert!(downloader.is_aliyun_oss_url(real_oss_url), "åº”è¯¥è¯†åˆ«ä¸ºé˜¿é‡Œäº‘ OSS URL");
        
        // æµ‹è¯•å…¶ä»–é˜¿é‡Œäº‘ OSS URL æ ¼å¼
        let test_cases = vec![
            ("https://bucket.oss-cn-hangzhou.aliyuncs.com/file.zip", true),
            ("https://my-bucket.oss-us-west-1.aliyuncs.com/path/file.tar.gz", true),
            ("https://test.oss-ap-southeast-1.aliyuncs.com/docker.zip", true),
            ("https://example.com/file.zip", false),
            ("https://github.com/user/repo/releases/download/v1.0.0/file.zip", false),
            ("ftp://bucket.oss-cn-beijing.aliyuncs.com/file.zip", false),
        ];
        
        for (url, expected) in test_cases {
            assert_eq!(
                downloader.is_aliyun_oss_url(url), 
                expected,
                "URL: {} åº”è¯¥è¿”å› {}",
                url, expected
            );
        }
    }
    
    #[test]
    fn test_downloader_type_detection() {
        let downloader = FileDownloader::default();
        
        // æµ‹è¯•æ‚¨çš„çœŸå® OSS URLï¼ˆå…¬ç½‘è®¿é—®ï¼‰
        let real_oss_url = "https://nuwa-packages.oss-rg-china-mainland.aliyuncs.com/duck-client-releases/docker/20250705082538/docker.zip";
        let downloader_type = downloader.get_downloader_type(real_oss_url);
        
        match downloader_type {
            DownloaderType::HttpExtendedTimeout => println!("âœ… æ­£ç¡®è¯†åˆ«ä¸ºæ‰©å±•è¶…æ—¶ HTTP ä¸‹è½½ï¼ˆå…¬ç½‘è®¿é—®ï¼‰"),
            DownloaderType::Http => println!("âŒ é”™è¯¯è¯†åˆ«ä¸ºæ™®é€š HTTP ä¸‹è½½"),
        }
        
        // å¯¹äºé˜¿é‡Œäº‘ OSS æ–‡ä»¶ï¼Œåº”è¯¥ä½¿ç”¨æ‰©å±•è¶…æ—¶HTTPä¸‹è½½
        assert!(matches!(
            downloader_type,
            DownloaderType::HttpExtendedTimeout
        ), "OSSæ–‡ä»¶åº”è¯¥ä½¿ç”¨æ‰©å±•è¶…æ—¶HTTPä¸‹è½½");
        
        // æµ‹è¯•æ™®é€š HTTP URL
        let http_url = "https://github.com/user/repo/releases/download/v1.0.0/file.zip";
        assert!(matches!(
            downloader.get_downloader_type(http_url),
            DownloaderType::Http
        ), "æ™®é€š HTTP URL åº”è¯¥ä½¿ç”¨æ ‡å‡†ä¸‹è½½");
    }
} 